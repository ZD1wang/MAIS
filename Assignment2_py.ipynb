{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_owfp8ZLMQIs"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZD1wang/MAIS/blob/master/Assignment2_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kjY0-ouKNkj",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2 ― Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_owfp8ZLMQIs",
        "colab_type": "text"
      },
      "source": [
        "## 0. Introduction\n",
        "\n",
        "In this second assigment, we will explore another cornerstone of machine learning: supervised classification. We will be specifically classifying movie reviews by their corresponding score. To do this, we will first pre-process the raw data by cleaning and turning each review into a vector. Then, we will explore the following learning algorithms for classification: support vector classifiers, random forests, and naive Bayes classifiers.\n",
        "\n",
        "* [Question 1.1](#scrollTo=qQPEFaRiRbOS)\n",
        "* [Question 2.1](#scrollTo=LawTBDrOcPJe)\n",
        "* [Question 2.2](#scrollTo=Hqp7LyWtlLVx)\n",
        "* [Question 3.1](#scrollTo=Qs_t1uVzh01s)\n",
        "* [Question 3.2](#scrollTo=fl9Z0VVXoRsY)\n",
        "* [Question 3.3](#scrollTo=Perywb8YapEU)\n",
        "* [Question 4.1](#scrollTo=YKLBuWjmAKoJ)\n",
        "* [Question 5.1](#scrollTo=Myn-42J9ACsH)\n",
        "* [Question 6.1](#scrollTo=fN-dse1NBQnk)\n",
        "* [Question 6.2](#scrollTo=wfCjr-JrEJya)\n",
        "* [Quesiton 6.3](#scrollTo=l1iGVZtkE5fF)\n",
        "$% latex commands for later use$\n",
        "$\\newcommand{\\R}{\\mathbb{R}}$\n",
        "$\\newcommand{\\B}{\\mathbb{B}}$\n",
        "$\\newcommand{\\argmax}{\\operatorname*{arg\\ max}}$\n",
        "$\\newcommand{\\given}{\\; \\vert \\;}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4J_3kZ-KaQw",
        "colab_type": "text"
      },
      "source": [
        "## 1. Importing Libraries and Data\n",
        "\n",
        "For this assignment, we will be using a dataset of metatritic reviews. The data consists of a csv file where the first column is a string containing a user review and the second column contains the corresponding score that the user gave to the movie. First, we will import any libraries that we might use.\n",
        "\n",
        "**Note.** You may use any library you'd like unless when otherwise specified to not use any or to use a particular one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQPEFaRiRbOS",
        "colab_type": "text"
      },
      "source": [
        "### Question 1.1 Importing Libraries\n",
        "\n",
        "Add any modules you use as you complete the assignment here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a40HohhQRalg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import random\n",
        "### Answer starts here ###    \n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy   \n",
        "import numpy as np\n",
        "\n",
        "from collections import Counter \n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "### Answer ends here ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp7IQzDYrVvl",
        "colab_type": "code",
        "outputId": "0c2ddd28-b03c-4d17-c861-ae03918ae530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/LiTigre/McGillAI-asg2-data/master/metacritic_dataset.csv"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-02 18:14:48--  https://raw.githubusercontent.com/LiTigre/McGillAI-asg2-data/master/metacritic_dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22960821 (22M) [text/plain]\n",
            "Saving to: ‘metacritic_dataset.csv.1’\n",
            "\n",
            "metacritic_dataset. 100%[===================>]  21.90M  31.8MB/s    in 0.7s    \n",
            "\n",
            "2020-02-02 18:14:49 (31.8 MB/s) - ‘metacritic_dataset.csv.1’ saved [22960821/22960821]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u8hq-QRohS6",
        "colab_type": "text"
      },
      "source": [
        "Let's create a variable to hold the name of the dataset for convenience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KiwMQsdogGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_filename = 'metacritic_dataset.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl87TKjnI558",
        "colab_type": "text"
      },
      "source": [
        "Also create a function to print a review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrX5YMhWJAOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_review(review, score):\n",
        "  print('--------------- Review with score of {} ---------------'.format(score))\n",
        "  print(review)\n",
        "  print('------------------------------------------------------')\n",
        "  print()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bzt1wY-DoLJR",
        "colab_type": "text"
      },
      "source": [
        "Let's load the data and see what the first 10 reviews look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biQkghO_kjCI",
        "colab_type": "code",
        "outputId": "6cc9977a-b5e9-4aef-8532-1749387372cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with open(dataset_filename) as csv_file:\n",
        "  csv_reader = csv.reader(csv_file)\n",
        "  colnames = next(csv_reader)\n",
        "  data = list(csv_reader)\n",
        "  \n",
        "for review, score in random.sample(data, 10):\n",
        "  print_review(review, score)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------- Review with score of 2 ---------------\n",
            "An interesting premise leads to some decent, though shallow situations, which lead to a pretty formulaic ending that feels like a TV movie, and many loose ends are left unsatisfied.\n",
            "\n",
            "I couldn't tell if I was watching a satirical poke at zombie movies or a rather mediocre serious take on zombie movies.  It seemed like it couldn't make up its mind.  It also seemed like it didn't know if it was aiming for adult audiences or college age audiences.  It sort of hovers in the middle, afraid to offend either party\n",
            "\n",
            "These characters are as paper-thin as they come.  Having all jumped from the pages of the conglomerated codex of all zombie movies put together, they feel no need to explain their motivations, since it clearly knows you've seen this shlock a hundred times before.  Some half-hearted attempts are made at backstory, but they are mostly of the 'tell and don't show' variety\n",
            "\n",
            "Bill Murray makes a rather odd appearance, and it just felt like it was saying \"We're cultured, we appreciate Bill Murray.\"  Just like many a college student, the movie can't help itself from trying to get your attention and adoration, stroking its own ego at the expense of being a good movie.  Would not see again, would not recommend\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 0 ---------------\n",
            "Let me start by saying I considered Goodfellas the best film of the past 30 years. Normally I would call Martin one of the top directors of the era  BUT this movie is terrible on every level. Gratuitous sexual humor that goes on forever to the point of it becoming a soft-core porno! Nothing remotely redeeming in any character. And I generally always come out of a Scorsese movie quoting at least a few great lines  I can't think of anything said in this movie at all. It is so bad that it now makes me wonder about Scorsese and his career in total! Be warned and skip this awful rubbish.\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 5 ---------------\n",
            "Elizabeth does just fret me,she is so stupid and spoil and have nothing better to do than just look hard.\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 5 ---------------\n",
            "I just really have to agree with the metacritic rating and other peoples thoughts on this movie. [Ed: you mean disagree?] Nine times out of ten I've thought that the metascore and the user vote were pretty dead on, with this film however I just have to disagree. I saw this film during my trip to New Zealand and I was just very disappointed. The jokes didn't really hit half the time. The never ending action near the end of the film just bored me about half way through. I thought, \"Ohhhh I get what they're trying to do here, making fun of movies like Bad Boys. Cute.\" Past that point I was just tired of it. The plot was just LAME. The ending was just plain bad. Again, I understand that they were making a farce of a genre, but you have to make that farce funny or at least interesting. It was only kinda funny and just completely uninteresting by the last 45 minutes of the movie. On top of that, if you have flashing light induced siezures do NOT see this movie. I felt like a camera bulb was flashing in my eyes through about have the movie. The editing was just BAD. I mean, I understand doing that flash during the scenes where the officer is booking people and taking their pictures with the numbers, but they did it for EVERY little thing. It really just gets to you. So in the end, it had moments that were funny, but overall I just can't recommend it.\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 4 ---------------\n",
            "SPOILERS!scroll down for review...This movie is highly polished nonsense.A meteor (or something) crashes to Earth and it collides with a light-house. This forms a little shimmery-thing they call \"the shimmer\"... Fast forward to 3 years later and there have been countless attempts to send people into the shimmer, but nobody has ever returned. They keep sending people in and the land that the shimmer covers continues to grow.There's even a scene where they say, in a matter of time it will spread across the state... then, the country... then, the WORLD.Our story revolves around a group of female scientists who enter \"the shimmer\". There mission is to reach the lighthouse at the centre of said phenomenon and return with \"data\". At this point of the movie, I immediately slammed the breaks... Because, I expect narratives to make sense... But I soldiered on, regardless, reluctantly offering yet another well-received Hollywood film the benefit of the doubt.Before long, we see them enter the shimmer.They walk to the edge of it and - then - they enter.Nobody escorts to them to the edge of \"the shimmer\".And, amazingly, it is transparent.At this point of the film, I knew without-a-shadow-of-a-doubt that I was watching a stupid film... I mean - SURELY! - if 3 years worth of expeditions have resulted in nobody returning from \"the shimmer\" and if it's transparent, then why in the hell wouldn't people be standing near the edge of it and closely observing others entering? Why not tie a chain around their waste and drag it back again?There is a really brief bit of dialogue that attempts to satisfy none of these questions. All we get is a comment about how they've sent drones in. That doesn't satisfy my scepticism. I'm not dying to immerse myself in special effects. I need a story that makes sense.So, anyway, moving on from that...They encounter a couple of really unimaginative monsters on their way to the lighthouse. There's a giant crocodile with layers of teeth, like a shark. And, there's a bear without a face that does an Alien (franchise) routine.The team they send in consists almost entirely of completely useless idiots. I thought an entirely female cast was supposed to be some sort of feminist statement? Would a comparable film ever exist with such incompetent and mentally unstable male protagonists?The film keeps cutting back to a sex scene between Natalie Portman and Oscar Isaac, for some reason.The moment they figure out what \"the shimmer\" is, is typical exposition-nonsense. The idea behind it is intriguing, but the film doesn't explore it. There are a couple of scenes were we see cells dividing, but the special effects aren't great (like with the giant crocodile) and it all looks a bit aged.Framing the narrative around the Asian guy in the haz-mat suit didn't work for me. It seemed really cliched and B-movie...I liked the first part of the ending, with the replica of Natalie Portman's character and the grenades. But, as the plot unravelled, it continued to ask more questions without answers. I liked Isaac's character coming back from the void. But, the end of the film is stupid.It's too complicated for it's own good.Why spend 3 years and create an ecosystem only to burn it down all so you can find Natalie Portman and Oscar Isaac?The ending is like the ending to a Twilight Zone episode or an instalment of Tales from the Crypt. You know those classic trash horror / sci-fi shows, were someone turns around (after an encounter with a monster) and their eyes glow red just as the credits role?That's this film.And, I don't have a problem with those sort of flicks.\n",
            "\n",
            "But - like a lot of modern films - Annihilation tries to pass itself off as clever by promising so much, and building audience expectations to such an extent, that we accept an anti-climax as a climax. Sort of like a cult, convincing it's followers that the leader is worthy of worship.\n",
            "\n",
            "The success of the TV show \"Lost\" has had a terrible impact on science fiction.\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 10 ---------------\n",
            "A great movie worth watching. Bradley Cooper is excellent both as an actor & first time director. Lady Gaga is an excellent vocalist and her acting efforts are not that bad!\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 10 ---------------\n",
            "Boyhood made me want to enjoy life, to get off the couch and really find what I want to do with my life. It made me want to get out of my house and meet new people. Thanks to this film I´m now really scared of dying knowing that there´s a lot for me to experience.\n",
            "I have never been able to relate to a story and its protagonists as much as I did with Boyhood and while some people may find it \"boring\" or \"too long\" but for me it was perfect. And the fact that they used the same actors for all the film just adds more to the awesome this film is.\n",
            "So thank you everyone who contributed for this. Thank you\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 9 ---------------\n",
            "Heartfelt, picturesque, surprisingly amazing, and filled with very memorable catchy soundtracks. This is what Disney's Frozen is made off. A newly engaging popcorn flick that tends not only visually but awesomely a fun time you will ever watched in this year.\n",
            "\n",
            "Rating: 3.5 / 4\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 6 ---------------\n",
            "On the good side, it is the second most expensive film ever made, and it looks like it. The cinematography is gorgeous, and Thomas Newman clearly had fun writing this score.\n",
            "\n",
            "On the bad side, the actual events that are being filmed aren't that interesting. The Bourne-style action and ruggedly humorless Daniel Craig characterization are more than a little tired, as is the series of references to Bond tropes. It isn't a great showing for the supporting cast either; Christoph Waltz and Monica Belucci might seem like casting coups, but they're both wasted, and Lea Seydoux is of course very, very pretty, but her character's archetypically fickle and irrational Bond girl behavior feels increasingly out of touch in a gritty 2010's movie.\n",
            "\n",
            "Plot holes abound, the henchmen have the level of marksmanship you'd expect, and you'll wonder what kind of intelligence agency builds a massive prominent headquarters out of glass. None of this is really outside of the Bond ouvre, but it's getting tired. It's a fun movie at times, but it's trying to be realistic, unrealistic, and tongue-in-cheek all at the same time, and that just doesn't work.\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 8 ---------------\n",
            "Related media: I have watched all of the previous seven Harry Potter films but have not read any of the books, including the Fantastic Beasts book.What's it like?: It's like the first Harry Potter film, where they introduce the World of Wizardry, its magic, human characters and animals, combined with the horrifying world of witch-hunting in the old US of A.Pros:1) As promised by the title, there are some fantastic beasts! 2) The film also brings the world of Harry Potter, its magic, human characters and animals, back into our consciousness again, but it is going in a totally different direction, and that is interesting and original.Cons:1) The obvious villain in the film, a very destructive force, could be better presented towards the end of the film. As in Alien and Jaws, less is more. There was too much of it in the film, yet it felt insufficiently explained. This part here could be shortened by few minutes. (Note: There is a less obvious villain in the film.)3) Johnny Depp's casting in this film felt like an attempt to promote his flagging career.How would the the different age groups rate it?Children: (The film is not rated for children.)Teens: ExcellentYoung adults: GoodMedium age adults: GoodOld adults: GoodRating: 4/5 (no half scores). It is worth spending your money and watching it in the cinemas.\n",
            "------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0hrZUOhKSKK",
        "colab_type": "text"
      },
      "source": [
        "## 2. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8POC4JhEknV1",
        "colab_type": "text"
      },
      "source": [
        " We will be converting our data into a binary bag-of-words representation (Google \"binary bag-of-words\"). To do this, we will perform two steps beforehand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LawTBDrOcPJe",
        "colab_type": "text"
      },
      "source": [
        "### Question 2.1 Cleaning the Data\n",
        "Create a function called `clean`, which takes a string and then:\n",
        "\n",
        " 1. lower-case all words \n",
        " 2. only keeps letters and spaces\n",
        "  \n",
        "  \n",
        "  For example, this will cause\n",
        "  \n",
        "  >`This was the WORST movie I have EVER SEEN!!!!!!`\n",
        "  \n",
        "  to become\n",
        "  \n",
        "  >`this was the worst movie i have ever seen`\n",
        "  \n",
        "   Of course, you could do more pre-processing steps if you would like, such as lemmatization, stemming, etc... but TOTALLY OPTIONAL!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofJ5ThHrH2jP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "def clean(review):\n",
        "  ### Start of Answer ###\n",
        "  lowerCase = review.lower()\n",
        "  case = re.sub(r'[^a-zA-z ]+', '', lowerCase)\n",
        "  global nlp\n",
        "  doc = nlp(case)\n",
        "  return \" \".join([token.lemma_ for token in doc])\n",
        "  ### End of Answer ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fXVoL083-nv",
        "colab_type": "text"
      },
      "source": [
        "Test your function with this example string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKIv1cTx39z-",
        "colab_type": "code",
        "outputId": "41d87ed5-f4bd-4ad2-a092-885880e03080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(clean(\"This was the WORST movie I have EVER SEEN!!!!!!\"))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this be the bad movie i have ever see\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLViwZ89Iv0Q",
        "colab_type": "text"
      },
      "source": [
        "Now, we'll use the function to clean the whole dataset. We'll also turn the scores into integers while we're at it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlIL4h1aIyUo",
        "colab_type": "code",
        "outputId": "3ad8b4d0-59fe-4772-a1dd-90351b698d4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "clean_data = [[clean(review), int(score)] for review, score in data]\n",
        "\n",
        "for review, score in random.sample(clean_data, 5):\n",
        "  print_review(review, score)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------- Review with score of 10 ---------------\n",
            "rent be so amazing i see -PRON- opening night and -PRON- be see -PRON- for the rd time this weekend all the cast do a fabulous job potraye -PRON- character sole all though i do not see the broadway production -PRON- have hear the movie be just as good or well rent be a heart warm movie that really show the importance for live life to the fulli and the love friend have for eachother idina menzel -PRON- idol be amazing like usual as well as the other cast memeber i recommend this movie to anyone\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 10 ---------------\n",
            "such great dialogue that -PRON- deserve an oscar   and make -PRON- realize how starved -PRON- be for intelligent dialogue in movie bradley cooper and jennifer lawrence have surprisingly good chemistry and this help -PRON- overcome the desire to occasionally slap -PRON- silly\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 1 ---------------\n",
            "there be very little to no story -PRON- be like run around in a circle for two and a half hour in the end nothing be resolve this be one long pointless protract trailer for pirate iii a movie by the way that i will not be see\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 9 ---------------\n",
            "silver lining playbook be why -PRON- go to the movie and i can not remember the last film that evoke such a spectrum of emotion i grab -PRON- early on and do not let go largely due to -PRON- brilliant performance a huge nod to jennifer lawrence and culturally relevant screenplay -PRON- will laugh cry and cheer during several of -PRON- dysfunctionally delicious segment and -PRON- will fall in love with at least a couple of -PRON- character back by an outstanding soundtrack slp be one of those film that -PRON- will recommend to everyone -PRON- know yes -PRON- that good and thank godnmovie like this be still be make -PRON- will most assuredly do well at this year academy award\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 8 ---------------\n",
            "ridley scott finally regain -PRON- footing with   of all thing   familiar territory the martian triumphant air and superior visual prowess in conjunction with matt damon unflappably charismatic performance make for something truly special\n",
            "------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqp7LyWtlLVx",
        "colab_type": "text"
      },
      "source": [
        "### Question 2.2 Picking features\n",
        "\n",
        "We now need to turn each review into vectors. We will pick the top 10,000 words as features\n",
        "\n",
        "Using those 10,000 features, create a function called `vectorize` which will take a string as an input, and convert it to a vector using the binary bag of words representation.\n",
        "\n",
        "For example, the string `\"This movie made me cry\"` will become a vector of size 10,000 with 5 elements being 1 (assuming each word is part of the 10,000 most common) and 9995 being 0, that it, is i will look something like\n",
        "\n",
        " > `[0, 0, ..., 0, 1, 0, ..., 0, 1, 0..., 0, 1, 0, ..., 0, 1, 0 ..., 0, 1, 0, ..., 0, 0]`\n",
        " \n",
        " In order to accomplish this task, you will\n",
        " \n",
        " 1. write a `get_vocab` function which takes as an argument a list of (cleaned) reviews and the vocabulary size and outputs the a list of size `vocab_size` containing the most common words.\n",
        " 2. write a `vectorize` function which takes as an argument a review and the vocabulary and turns the review into its binary bag of words representation.\n",
        " 3. use the `vectorize` function to create a new variable called `vectorized_data` which will contain the bag-of-words representation of each data point contained in the `clean_data` variable rather than its string representation. Don't forget that a data point consists of review-score pairs.\n",
        "\n",
        "**Warning**: this may take up to five minutes depending on implementation. Make use of dictionaries if you want this to be faster. However, speed is not evaluated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOJqbGQkzE7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vocab(reviews, vocab_size):\n",
        "  all_words = []\n",
        "  for review in reviews:\n",
        "    review = review.split()\n",
        "    for index in range(len(review)):\n",
        "      all_words.append(review[index])\n",
        "\n",
        "  count = Counter(all_words)\n",
        "  return count.most_common(vocab_size)\n",
        "\n",
        "# def vectorize_all(reviews, vocab):\n",
        "#   all_bags = []\n",
        "#   for review in reviews:\n",
        "#     bag_vector = [vectorize(review[0], vocab), review[1]]\n",
        "#     all_bags.append(bag_vector)\n",
        "#   return all_bags\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dizb-QL7OLu",
        "colab_type": "text"
      },
      "source": [
        "Test your function with the following code.\n",
        "The `vocabulary` variable should have a length of 2000 and the most common words should be \"the\", \"and\", \"a\", etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shnihj2ouIaR",
        "colab_type": "code",
        "outputId": "4e62229b-239b-4da4-b8f8-561d85322d51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "num_features = 2000\n",
        "vocabulary = get_vocab([review for review, score in clean_data], num_features)\n",
        "print(len(vocabulary))\n",
        "print(vocabulary)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n",
            "[('the', 234698), ('-PRON-', 225589), ('be', 188970), ('and', 119354), ('a', 108975), ('of', 95374), ('to', 86201), ('movie', 62646), ('i', 57536), ('this', 55318), ('in', 53398), ('not', 52226), ('that', 52096), ('have', 42839), ('film', 36368), ('but', 33889), ('do', 33527), ('for', 30085), ('with', 29837), ('as', 29253), ('good', 23339), ('one', 18333), ('on', 18177), ('like', 17726), ('all', 17564), ('see', 17546), ('make', 15834), ('just', 15049), ('an', 14831), ('character', 14623), ('so', 14480), ('at', 14128), ('great', 13730), ('from', 13697), ('story', 12897), ('well', 12169), ('there', 12159), ('by', 11992), ('what', 11965), ('more', 11800), ('time', 11631), ('get', 11503), ('if', 11462), ('some', 11147), ('about', 11085), ('very', 10995), ('can', 10914), ('really', 10763), ('go', 10757), ('or', 10644), ('will', 10357), ('out', 10132), ('would', 9689), ('watch', 9531), ('s', 9381), ('who', 9285), ('action', 9036), ('think', 8958), ('no', 8635), ('bad', 8266), ('scene', 8229), ('than', 8188), ('much', 8049), ('up', 7928), ('even', 7829), ('first', 7720), ('feel', 7620), ('love', 7541), ('say', 7339), ('when', 7253), ('which', 7104), ('only', 7060), ('how', 6900), ('give', 6855), ('most', 6849), ('because', 6720), ('other', 6603), ('plot', 6552), ('way', 6427), ('could', 6374), ('people', 6280), ('also', 6207), ('too', 6119), ('end', 6087), ('into', 6046), ('thing', 5983), ('know', 5724), ('take', 5679), ('look', 5295), ('come', 5141), ('still', 4913), ('ever', 4904), ('any', 4808), ('many', 4789), ('want', 4764), ('year', 4752), ('then', 4563), ('mom', 4260), ('after', 4250), ('lot', 4239), ('find', 4216), ('performance', 4169), ('never', 4130), ('spaghetti', 4118), ('little', 4092), ('while', 4071), ('should', 4051), ('life', 4033), ('part', 4005), ('may', 3965), ('work', 3960), ('show', 3946), ('seem', 3910), ('over', 3857), ('why', 3826), ('where', 3805), ('book', 3775), ('enjoy', 3746), ('world', 3745), ('fun', 3701), ('act', 3686), ('every', 3683), ('two', 3669), ('effect', 3630), ('new', 3621), ('man', 3615), ('try', 3548), ('funny', 3539), ('amazing', 3531), ('fan', 3516), ('off', 3416), ('actor', 3387), ('big', 3367), ('play', 3350), ('original', 3330), ('something', 3325), ('acting', 3316), ('though', 3262), ('these', 3239), ('leave', 3197), ('moment', 3181), ('long', 3099), ('again', 3079), ('those', 3064), ('pretty', 3038), ('cast', 3034), ('nothing', 3031), ('point', 3026), ('bit', 3006), ('same', 2982), ('back', 2968), ('here', 2945), ('critic', 2913), ('use', 2912), ('start', 2896), ('such', 2888), ('review', 2882), ('expect', 2822), ('another', 2792), ('actually', 2780), ('need', 2770), ('real', 2769), ('everything', 2743), ('through', 2740), ('far', 2676), ('now', 2653), ('director', 2649), ('however', 2607), ('few', 2603), ('enough', 2596), ('keep', 2562), ('interesting', 2541), ('whole', 2488), ('old', 2474), ('before', 2463), ('tell', 2460), ('perfect', 2452), ('last', 2439), ('overall', 2432), ('right', 2425), ('minute', 2402), ('down', 2386), ('happen', 2365), ('between', 2353), ('visual', 2340), ('both', 2326), ('without', 2302), ('quite', 2280), ('horror', 2267), ('audience', 2258), ('war', 2253), ('kid', 2245), ('star', 2238), ('hour', 2214), ('guy', 2199), ('day', 2186), ('mean', 2182), ('set', 2172), ('almost', 2172), ('fight', 2165), ('dark', 2163), ('comedy', 2142), ('de', 2139), ('series', 2130), ('special', 2125), ('put', 2121), ('understand', 2107), ('own', 2098), ('script', 2098), ('job', 2072), ('sense', 2071), ('become', 2070), ('fantastic', 2061), ('yet', 2046), ('fact', 2040), ('main', 2036), ('bring', 2015), ('write', 2010), ('marvel', 2007), ('role', 2003), ('second', 1997), ('anything', 1995), ('let', 1987), ('everyone', 1983), ('since', 1982), ('villain', 1974), ('especially', 1961), ('turn', 1947), ('different', 1944), ('definitely', 1943), ('sure', 1925), ('probably', 1918), ('laugh', 1902), ('must', 1902), ('dialogue', 1901), ('worth', 1901), ('sequel', 1883), ('human', 1879), ('music', 1864), ('always', 1862), ('hard', 1855), ('around', 1848), ('score', 1836), ('least', 1831), ('believe', 1823), ('line', 1821), ('idea', 1820), ('reason', 1817), ('sequence', 1805), ('ending', 1799), ('awesome', 1796), ('screen', 1785), ('read', 1781), ('away', 1776), ('kind', 1773), ('game', 1769), ('high', 1767), ('lack', 1759), ('place', 1758), ('true', 1720), ('humor', 1718), ('kill', 1709), ('truly', 1684), ('hope', 1683), ('create', 1681), ('problem', 1680), ('excellent', 1679), ('beautiful', 1676), ('hate', 1673), ('bore', 1670), ('once', 1655), ('superhero', 1648), ('lead', 1641), ('absolutely', 1622), ('live', 1619), ('completely', 1611), ('experience', 1604), ('change', 1598), ('rather', 1592), ('simply', 1583), ('franchise', 1580), ('favorite', 1579), ('mind', 1564), ('top', 1562), ('half', 1561), ('joke', 1559), ('follow', 1558), ('brilliant', 1553), ('anyone', 1543), ('maybe', 1543), ('comic', 1537), ('fall', 1520), ('better', 1518), ('cgi', 1508), ('care', 1504), ('instead', 1502), ('recommend', 1491), ('family', 1487), ('although', 1481), ('each', 1480), ('entertaining', 1466), ('terrible', 1464), ('move', 1463), ('classic', 1442), ('lose', 1435), ('child', 1435), ('stupid', 1431), ('la', 1425), ('less', 1419), ('together', 1404), ('help', 1401), ('alien', 1391), ('animation', 1385), ('add', 1377), ('entertain', 1372), ('save', 1371), ('hero', 1361), ('miss', 1361), ('next', 1360), ('wait', 1343), ('call', 1342), ('yes', 1338), ('spiderman', 1338), ('piece', 1337), ('pixar', 1335), ('wrong', 1334), ('entire', 1331), ('direct', 1330), ('full', 1328), ('money', 1311), ('deliver', 1304), ('enjoyable', 1300), ('throughout', 1298), ('else', 1292), ('waste', 1286), ('incredible', 1277), ('theater', 1272), ('john', 1268), ('talk', 1262), ('friend', 1259), ('young', 1244), ('name', 1242), ('twist', 1235), ('masterpiece', 1223), ('black', 1222), ('cool', 1216), ('direction', 1213), ('final', 1213), ('emotional', 1210), ('girl', 1210), ('strong', 1201), ('face', 1195), ('nice', 1194), ('deserve', 1188), ('rest', 1183), ('opinion', 1180), ('batman', 1178), ('monster', 1168), ('toy', 1167), ('american', 1164), ('begin', 1162), ('battle', 1162), ('theme', 1158), ('base', 1156), ('along', 1155), ('shrek', 1153), ('either', 1148), ('slow', 1145), ('run', 1141), ('bond', 1139), ('course', 1137), ('emotion', 1136), ('boring', 1136), ('d', 1135), ('que', 1135), ('until', 1134), ('genre', 1129), ('stand', 1109), ('cinematography', 1108), ('heart', 1107), ('flick', 1106), ('fail', 1104), ('word', 1104), ('viewer', 1101), ('someone', 1098), ('force', 1097), ('die', 1095), ('three', 1089), ('decent', 1086), ('despite', 1076), ('eye', 1073), ('already', 1073), ('solid', 1071), ('epic', 1068), ('style', 1066), ('woman', 1064), ('hollywood', 1062), ('certainly', 1061), ('wonder', 1057), ('development', 1056), ('past', 1052), ('age', 1049), ('sound', 1048), ('short', 1043), ('stop', 1041), ('scary', 1036), ('picture', 1032), ('version', 1022), ('during', 1018), ('soundtrack', 1015), ('manage', 1015), ('element', 1012), ('cinema', 1010), ('previous', 1009), ('suppose', 1000), ('level', 999), ('trilogy', 992), ('storyline', 991), ('oscar', 989), ('fine', 984), ('hear', 983), ('consider', 982), ('hilarious', 981), ('huge', 978), ('close', 972), ('drama', 972), ('future', 971), ('harry', 971), ('question', 969), ('include', 966), ('voice', 965), ('power', 957), ('finally', 954), ('head', 950), ('hand', 946), ('wonderful', 945), ('violence', 945), ('car', 942), ('attempt', 937), ('feature', 930), ('michael', 930), ('remember', 928), ('tom', 925), ('message', 924), ('easily', 922), ('awful', 920), ('xman', 920), ('death', 915), ('shoot', 909), ('poor', 908), ('seriously', 908), ('event', 906), ('side', 901), ('expectation', 901), ('deep', 900), ('god', 899), ('focus', 897), ('art', 896), ('person', 895), ('scifi', 895), ('feeling', 894), ('hold', 893), ('night', 893), ('credit', 890), ('highly', 889), ('hit', 888), ('predictable', 888), ('rate', 881), ('walk', 881), ('pace', 880), ('extremely', 879), ('animate', 876), ('weak', 875), ('disney', 874), ('boy', 871), ('universe', 869), ('simple', 867), ('compare', 865), ('pay', 862), ('avenger', 861), ('history', 860), ('quality', 857), ('view', 853), ('unique', 850), ('song', 850), ('evil', 850), ('super', 850), ('stay', 848), ('rating', 845), ('perfectly', 839), ('totally', 838), ('trailer', 835), ('build', 833), ('able', 824), ('guess', 823), ('third', 822), ('surprise', 820), ('thriller', 819), ('beginning', 816), ('zombie', 816), ('shot', 814), ('present', 813), ('grow', 812), ('stuff', 811), ('horrible', 809), ('adult', 807), ('ok', 806), ('oh', 805), ('sometimes', 804), ('forget', 803), ('writing', 800), ('exciting', 797), ('exactly', 797), ('matter', 796), ('against', 794), ('spend', 794), ('mention', 792), ('cut', 788), ('break', 786), ('attention', 786), ('win', 784), ('due', 784), ('ask', 783), ('release', 783), ('low', 783), ('iron', 783), ('thrill', 782), ('relationship', 781), ('hell', 781), ('behind', 778), ('fill', 778), ('wish', 776), ('support', 774), ('issue', 766), ('peter', 766), ('often', 765), ('couple', 765), ('sit', 765), ('potter', 765), ('dead', 762), ('small', 761), ('example', 758), ('portray', 757), ('average', 754), ('tone', 750), ('flaw', 746), ('camera', 746), ('serious', 745), ('detail', 743), ('video', 742), ('incredibly', 741), ('entertainment', 741), ('perhaps', 740), ('adaptation', 738), ('return', 737), ('later', 736), ('home', 733), ('meet', 733), ('smart', 732), ('scare', 731), ('ridiculous', 730), ('space', 727), ('involve', 727), ('sort', 726), ('disappoint', 725), ('aspect', 723), ('explain', 721), ('honestly', 720), ('fast', 719), ('america', 716), ('decide', 716), ('adventure', 716), ('thor', 715), ('complete', 713), ('musical', 711), ('ill', 704), ('throw', 703), ('mostly', 702), ('ride', 699), ('design', 697), ('except', 696), ('realistic', 694), ('earth', 694), ('open', 693), ('jackson', 690), ('sad', 687), ('unfortunately', 687), ('visually', 686), ('develop', 685), ('single', 684), ('blow', 683), ('powerful', 678), ('case', 675), ('provide', 674), ('thank', 673), ('reality', 670), ('stunning', 669), ('depth', 668), ('amount', 668), ('fantasy', 667), ('learn', 666), ('light', 666), ('king', 666), ('important', 665), ('somewhat', 662), ('major', 662), ('captain', 662), ('suck', 661), ('mess', 661), ('tv', 660), ('intense', 660), ('summer', 658), ('concept', 657), ('production', 657), ('type', 657), ('touch', 656), ('middle', 656), ('y', 656), ('godzilla', 656), ('modern', 655), ('happy', 655), ('cliche', 652), ('impossible', 652), ('prove', 650), ('fit', 649), ('un', 648), ('crap', 646), ('david', 646), ('basically', 645), ('beyond', 645), ('hole', 642), ('impressive', 641), ('realize', 639), ('superman', 639), ('cause', 638), ('easy', 637), ('writer', 634), ('certain', 631), ('dumb', 630), ('please', 630), ('brother', 625), ('offer', 624), ('memorable', 623), ('clever', 622), ('father', 622), ('superb', 619), ('okay', 618), ('appreciate', 616), ('drive', 615), ('pull', 614), ('doubt', 609), ('finish', 608), ('suspense', 607), ('en', 606), ('son', 604), ('material', 604), ('situation', 604), ('tension', 604), ('negative', 601), ('mr', 598), ('obviously', 597), ('near', 597), ('allow', 597), ('several', 596), ('novel', 596), ('clearly', 595), ('science', 595), ('form', 593), ('dull', 592), ('possible', 592), ('park', 591), ('nolan', 591), ('cinematic', 590), ('tale', 589), ('white', 589), ('pacing', 586), ('early', 584), ('mark', 584), ('capture', 584), ('stick', 583), ('ruin', 581), ('general', 579), ('clear', 579), ('result', 578), ('appear', 577), ('nearly', 577), ('spectacular', 575), ('deal', 573), ('none', 572), ('under', 570), ('continue', 570), ('fiction', 570), ('forward', 568), ('cheesy', 567), ('mission', 567), ('choice', 565), ('similar', 563), ('disappointing', 562), ('alone', 559), ('chris', 555), ('wife', 555), ('did', 554), ('ring', 553), ('etc', 552), ('blockbuster', 551), ('obvious', 549), ('drag', 549), ('mix', 548), ('effort', 546), ('et', 546), ('team', 542), ('term', 542), ('jump', 542), ('dream', 540), ('school', 538), ('tarantino', 538), ('towards', 537), ('parent', 537), ('ultimately', 537), ('e', 537), ('literally', 536), ('opening', 536), ('city', 535), ('reference', 535), ('order', 534), ('particularly', 534), ('premise', 532), ('possibly', 531), ('number', 531), ('comic_strip', 530), ('chase', 529), ('hype', 529), ('agree', 528), ('jurassic', 528), ('note', 528), ('rush', 527), ('actual', 526), ('speak', 525), ('strange', 525), ('silly', 524), ('buy', 524), ('coraline', 524), ('fear', 523), ('typical', 523), ('cruise', 523), ('believable', 522), ('draw', 521), ('narrative', 521), ('screenplay', 520), ('positive', 518), ('romance', 518), ('surprisingly', 517), ('wow', 517), ('usual', 517), ('engage', 517), ('plus', 515), ('plenty', 514), ('weird', 512), ('sex', 512), ('beautifully', 509), ('spoiler', 508), ('robot', 508), ('introduce', 507), ('above', 507), ('whether', 506), ('chemistry', 504), ('planet', 504), ('potential', 503), ('date', 502), ('pure', 502), ('mother', 501), ('within', 501), ('flat', 500), ('blood', 499), ('outstanding', 498), ('group', 498), ('jame', 494), ('slightly', 490), ('nature', 489), ('spielberg', 489), ('cry', 488), ('usually', 488), ('remain', 487), ('steal', 487), ('atmosphere', 487), ('catch', 486), ('pack', 485), ('seat', 485), ('soon', 483), ('pick', 482), ('today', 481), ('scream', 481), ('title', 479), ('source', 479), ('reveal', 479), ('es', 479), ('complex', 478), ('humour', 477), ('edge', 476), ('large', 476), ('dramatic', 475), ('protagonist', 475), ('magic', 475), ('standard', 474), ('disappointed', 474), ('matrix', 474), ('interest', 473), ('adam', 473), ('disappointment', 473), ('intelligent', 472), ('giant', 472), ('poorly', 472), ('check', 472), ('remind', 471), ('remake', 471), ('house', 470), ('chance', 470), ('personally', 469), ('installment', 467), ('late', 466), ('destroy', 466), ('struggle', 465), ('award', 463), ('addition', 463), ('genius', 462), ('mediocre', 462), ('across', 461), ('bear', 460), ('conclusion', 460), ('unlike', 459), ('value', 458), ('state', 457), ('worthy', 456), ('mcu', 455), ('aside', 454), ('exist', 453), ('shame', 453), ('violent', 452), ('difficult', 452), ('purpose', 449), ('absolute', 448), ('knight', 448), ('answer', 448), ('studio', 446), ('sorry', 446), ('upon', 446), ('five', 445), ('lord', 445), ('rock', 444), ('creative', 444), ('green', 444), ('honest', 443), ('quickly', 443), ('damn', 443), ('yeah', 443), ('soldier', 443), ('recent', 442), ('fresh', 442), ('otherwise', 442), ('jack', 441), ('le', 441), ('smith', 441), ('inside', 440), ('figure', 439), ('million', 439), ('suffer', 439), ('personal', 438), ('attack', 438), ('plan', 438), ('creepy', 438), ('imagine', 437), ('o', 437), ('favourite', 436), ('respect', 436), ('personality', 434), ('creature', 434), ('avoid', 432), ('bay', 431), ('crazy', 430), ('baby', 428), ('apart', 428), ('hulk', 428), ('ago', 427), ('serve', 427), ('best', 426), ('describe', 426), ('terminator', 426), ('somehow', 425), ('scott', 424), ('drug', 424), ('bill', 424), ('wolverine', 424), ('transformer', 422), ('female', 420), ('suit', 419), ('annoying', 419), ('carry', 417), ('list', 416), ('graphic', 414), ('mystery', 413), ('compelling', 413), ('beat', 413), ('memory', 412), ('thought', 412), ('talent', 412), ('notice', 411), ('kick', 411), ('brain', 411), ('ten', 410), ('total', 409), ('trek', 408), ('choose', 407), ('admit', 405), ('color', 405), ('anyway', 403), ('room', 403), ('rise', 403), ('journey', 403), ('pointless', 402), ('costume', 402), ('reach', 402), ('ability', 402), ('appeal', 401), ('garbage', 401), ('origin', 401), ('edit', 401), ('tear', 400), ('dvd', 399), ('del', 399), ('el', 399), ('dinosaur', 398), ('budget', 398), ('episode', 398), ('body', 397), ('train', 397), ('beauty', 397), ('surprised', 396), ('decade', 396), ('predecessor', 395), ('escape', 395), ('actress', 395), ('emotionally', 394), ('praise', 393), ('craft', 392), ('background', 391), ('bland', 390), ('justice', 390), ('kinda', 389), ('ben', 387), ('pass', 386), ('una', 386), ('depp', 386), ('spot', 385), ('box', 385), ('four', 385), ('deadpool', 384), ('control', 382), ('lame', 382), ('fly', 381), ('treat', 381), ('shine', 381), ('bunch', 380), ('listen', 379), ('gore', 379), ('filmmaker', 379), ('class', 379), ('entirely', 378), ('step', 378), ('burton', 376), ('explore', 376), ('con', 375), ('setting', 375), ('ignore', 375), ('bruce', 374), ('gun', 374), ('fire', 373), ('sam', 373), ('kong', 372), ('charm', 371), ('fully', 371), ('robert', 371), ('race', 371), ('straight', 370), ('christian', 370), ('loud', 369), ('hurt', 369), ('mad', 369), ('member', 368), ('roll', 368), ('explosion', 367), ('daniel', 367), ('lifeshrek', 367), ('terrific', 366), ('familiar', 366), ('th', 365), ('dragon', 364), ('comment', 364), ('nor', 363), ('inspire', 363), ('society', 363), ('jennifer', 363), ('damon', 363), ('shock', 362), ('revenge', 362), ('unnecessary', 360), ('particular', 359), ('social', 358), ('ass', 357), ('romantic', 357), ('glad', 357), ('execute', 356), ('approach', 356), ('balance', 355), ('water', 355), ('ryan', 355), ('portrayal', 354), ('political', 354), ('produce', 353), ('climax', 353), ('self', 353), ('succeed', 352), ('theatre', 352), ('m', 351), ('reviewer', 351), ('culture', 351), ('soul', 351), ('subtle', 350), ('secret', 350), ('interested', 349), ('bother', 349), ('free', 348), ('cartoon', 348), ('barely', 347), ('se', 346), ('survive', 345), ('send', 345), ('period', 345), ('among', 345), ('further', 344), ('receive', 343), ('woody', 342), ('conflict', 342), ('whose', 342), ('motion', 342), ('b', 342), ('whatever', 341), ('connect', 341), ('travel', 341), ('difference', 340), ('contain', 338), ('christopher', 338), ('land', 337), ('stone', 336), ('hide', 336), ('generic', 335), ('subject', 335), ('cooper', 335), ('tim', 334), ('success', 334), ('pop', 334), ('truth', 334), ('paul', 334), ('crime', 333), ('mistake', 332), ('country', 332), ('vision', 332), ('discover', 332), ('island', 332), ('humanity', 331), ('likely', 331), ('impact', 330), ('jone', 330), ('confusing', 330), ('cheap', 330), ('project', 329), ('daughter', 329), ('career', 328), ('match', 328), ('directing', 328), ('lie', 328), ('lee', 326), ('dc', 326), ('hobbit', 326), ('william', 325), ('matt', 325), ('moral', 324), ('lawrence', 324), ('dicaprio', 324), ('thoroughly', 323), ('ground', 322), ('decision', 322), ('complain', 322), ('minor', 322), ('push', 321), ('door', 321), ('combine', 321), ('computer', 320), ('plain', 320), ('drop', 318), ('zero', 318), ('handle', 318), ('los', 318), ('fairly', 317), ('wall', 316), ('johnny', 315), ('taste', 313), ('t', 313), ('reboot', 313), ('complaint', 313), ('stark', 312), ('key', 311), ('brilliantly', 310), ('apparently', 310), ('rare', 310), ('multiple', 310), ('meaning', 309), ('stage', 309), ('pirate', 309), ('x', 309), ('outside', 308), ('rule', 307), ('user', 307), ('brad', 306), ('motivation', 306), ('red', 305), ('represent', 305), ('ship', 304), ('tie', 304), ('sell', 303), ('bale', 303), ('machine', 303), ('unless', 302), ('popcorn', 302), ('lady', 302), ('utterly', 302), ('intriguing', 302), ('crew', 302), ('regard', 302), ('eat', 302), ('spoil', 301), ('cover', 301), ('shyamalan', 301), ('imagination', 300), ('relate', 300), ('magical', 300), ('brutal', 298), ('trash', 298), ('george', 298), ('dialog', 298), ('besides', 297), ('suggest', 297), ('office', 296), ('share', 295), ('comedic', 295), ('week', 294), ('rich', 294), ('craig', 294), ('tense', 293), ('street', 293), ('raise', 292), ('queen', 292), ('air', 291), ('punch', 291), ('trust', 290), ('mile', 289), ('witch', 289), ('normal', 288), ('everybody', 287), ('heavy', 287), ('hardly', 287), ('childhood', 286), ('effective', 286), ('pitt', 286), ('wear', 286), ('hot', 286), ('ton', 286), ('image', 286), ('sadly', 285), ('forgettable', 285), ('les', 285), ('basic', 285), ('perspective', 284), ('spirit', 283), ('odd', 282), ('overrated', 282), ('phenomenal', 281), ('technology', 278), ('dr', 278), ('unexpected', 278), ('shallow', 278), ('blend', 278), ('confuse', 277), ('constantly', 276), ('pelcula', 276), ('hank', 275), ('teen', 275), ('casting', 275), ('connection', 275), ('tony', 275), ('length', 274), ('funniest', 274), ('satisfy', 274), ('vs', 274), ('coen', 274), ('generation', 273), ('guardian', 273), ('master', 273), ('intelligence', 273), ('language', 273), ('dislike', 272), ('comparison', 272), ('impress', 272), ('neither', 271), ('disaster', 270), ('indeed', 270), ('establish', 270), ('improve', 270), ('generally', 270), ('twice', 269), ('stunt', 269), ('sweet', 269), ('overly', 268), ('massive', 268), ('promise', 268), ('jackman', 268), ('root', 267), ('smile', 267), ('predator', 267), ('longer', 266), ('achieve', 266), ('substance', 265), ('fault', 265), ('fair', 265), ('witty', 264), ('forever', 264), ('prequel', 264), ('building', 264), ('ape', 264), ('awkward', 263), ('saga', 262), ('ghost', 261), ('army', 261), ('jar', 261), ('critical', 260), ('track', 260), ('dollar', 260), ('sign', 260), ('strike', 260), ('pg', 259), ('rip', 259), ('jim', 259), ('necessary', 259), ('genuinely', 259), ('content', 258), ('badly', 258), ('satisfying', 258), ('successful', 258), ('scale', 258), ('rely', 258), ('joy', 257), ('unbelievable', 257), ('judge', 257), ('random', 257), ('convince', 256), ('nowhere', 256), ('professional', 256), ('killer', 256), ('charming', 256), ('dog', 255), ('animal', 255), ('spy', 255), ('thin', 255), ('cold', 254), ('front', 254), ('strength', 254), ('fincher', 254), ('touching', 253), ('require', 253), ('gorgeous', 253), ('military', 253), ('century', 252), ('artistic', 252), ('public', 252), ('scorsese', 252), ('tired', 251), ('fake', 251), ('nobody', 251), ('bored', 251), ('exception', 250), ('nail', 250), ('annoy', 250), ('excited', 250), ('editing', 249), ('toro', 249), ('flawless', 249), ('dance', 249), ('anymore', 249), ('idiot', 248), ('alive', 248), ('[', 248), ('rent', 248), ('beast', 248), ('parker', 248), ('logan', 248), ('ready', 246), ('accept', 245), ('flow', 245), ('spectacle', 245), ('metacritic', 245), ('eventually', 245), ('grip', 244), ('arc', 244), ('eve', 244), ('grab', 243), ('desire', 243), ('par', 243), ('hey', 242), ('common', 242), ('western', 242), ('fox', 242), ('affleck', 242), ('galaxy', 242), ('st', 241), ('core', 241), ('finale', 241), ('quick', 241), ('entry', 241), ('l', 241), ('max', 241), ('pleasantville', 241), (']', 240), ('singer', 240), ('_', 240), ('male', 240), ('avatar', 240), ('skill', 240), ('claim', 239), ('mainly', 239), ('notch', 238), ('york', 238), ('cg', 238), ('highlight', 238), ('explanation', 238), ('formula', 238), ('bourne', 238), ('intend', 237), ('agent', 237), ('filme', 237), ('cute', 236), ('criticism', 236), ('medium', 236), ('steve', 236), ('blade', 236), ('insult', 235), ('individual', 235), ('joker', 234), ('cop', 234), ('murder', 234), ('unrealistic', 234), ('hugh', 234), ('immediately', 233), ('excitement', 233), ('howard', 233), ('pretentious', 233), ('depict', 232), ('extra', 232), ('blue', 232), ('enemy', 231), ('majority', 231), ('dude', 231), ('count', 231), ('hunt', 231), ('um', 231), ('mutant', 231), ('appearance', 230), ('various', 230), ('mood', 230), ('bright', 229), ('bloody', 229), ('portman', 229), ('girlfriend', 229), ('company', 228), ('surprising', 228), ('extreme', 228), ('redeem', 228), ('bug', 227), ('display', 227), ('likable', 227), ('surely', 227), ('r', 227), ('challenge', 227), ('carter', 227), ('jason', 227), ('kevin', 226), ('historical', 226), ('nick', 226), ('talented', 225), ('prefer', 225), ('underrated', 225), ('era', 224), ('viewing', 224), ('producer', 224), ('da', 224), ('gritty', 224), ('nemo', 224), ('party', 223), ('thus', 223), ('repeat', 223), ('cameo', 223), ('laughable', 223), ('gaga', 223), ('structure', 221), ('robin', 221), ('energy', 221), ('venom', 221), ('downey', 221), ('affect', 220), ('excuse', 220), ('eastwood', 220), ('realism', 219), ('presence', 219), ('superior', 219), ('glass', 219), ('antman', 219), ('alot', 218), ('weapon', 218), ('rd', 218), ('natural', 218), ('mature', 217), ('equally', 217), ('pain', 217), ('storytelle', 217), ('logic', 217), ('area', 217), ('imax', 217), ('execution', 216), ('dawn', 216), ('u', 216), ('famous', 215), ('bar', 215), ('adapt', 215), ('club', 215), ('fighting', 215), ('storytelling', 214), ('product', 214), ('c', 214), ('mysterious', 214), ('british', 214), ('como', 214), ('copy', 214), ('popular', 214), ('crash', 213), ('onto', 213), ('cult', 213), ('grand', 213), ('clich', 213), ('enter', 213), ('torture', 212), ('mais', 212), ('trick', 212), ('gibson', 212), ('suspenseful', 212), ('perform', 212), ('solo', 212), ('jeff', 212), ('sick', 211), ('lesson', 211), ('mixed', 211), ('parody', 211), ('min', 211), ('passion', 211), ('danger', 211), ('sheer', 211), ('bridge', 211), ('ticket', 211), ('infinity', 210), ('frame', 210), ('terror', 210), ('wonderfully', 210), ('anime', 209), ('constant', 209), ('skip', 209), ('post', 209), ('lazy', 209), ('nonsense', 208), ('relief', 208), ('opportunity', 208), ('speed', 208), ('loveshrek', 208), ('environment', 207), ('location', 207), ('fashion', 207), ('interpretation', 206), ('lover', 206), ('jr', 206), ('scientist', 206), ('dad', 206), ('safe', 205), ('concern', 205), ('japanese', 205), ('apocalypse', 205), ('accurate', 204), ('anderson', 204), ('willis', 204), ('cost', 204), ('nd', 203), ('originality', 203), ('identity', 203), ('wrap', 203), ('psychological', 203), ('tree', 203), ('academy', 202), ('replace', 202), ('afraid', 202), ('provoke', 202), ('join', 201), ('knowledge', 201), ('target', 201), ('english', 201), ('sake', 201), ('deeply', 201), ('rarely', 201), ('blast', 201), ('price', 201), ('failure', 201), ('road', 201), ('carrey', 200), ('scenery', 200), ('month', 200), ('humorous', 200), ('prometheus', 200), ('tough', 199), ('town', 199), ('mouth', 199), ('jam', 199), ('assume', 199), ('por', 199), ('essentially', 198), ('trip', 198), ('martin', 198), ('slowly', 197), ('gang', 197), ('cat', 196), ('being', 196), ('till', 195), ('trouble', 195), ('disturb', 195), ('overthetop', 195), ('current', 195), ('painful', 195), ('bomb', 195), ('accent', 194), ('loose', 194), ('sum', 194), ('technical', 193), ('teenage', 193), ('frankly', 193), ('breath', 193), ('factor', 193), ('mindless', 193), ('convey', 193), ('alan', 193), ('samuel', 193), ('gay', 193), ('asleep', 193), ('ridley', 193), ('buzz', 192), ('dangerous', 192), ('process', 192), ('aim', 191), ('hint', 191), ('jesus', 191), ('food', 191), ('flash', 191), ('runner', 191), ('genuine', 190), ('sister', 190), ('iconic', 190), ('captivate', 190), ('business', 190), ('regret', 190), ('maker', 190), ('steven', 190), ('snyder', 190), ('study', 189), ('whilst', 189), ('crowd', 189), ('somewhere', 189), ('seemingly', 189), ('rescue', 189), ('ball', 189), ('chapter', 189), ('angry', 188), ('dan', 188), ('lo', 188), ('threat', 188), ('reeve', 188), ('phoenix', 188), ('edward', 187), ('suddenly', 187), ('lotr', 187), ('princess', 187), ('mike', 187), ('hopefully', 187), ('animated', 186), ('nicely', 186), ('express', 186), ('backstory', 186), ('impression', 186), ('f', 186), ('burn', 186), ('risk', 186), ('exposition', 186), ('nonetheless', 186), ('conversation', 186), ('mary', 186), ('doctor', 186), ('ii', 186), ('patrick', 185), ('para', 185), ('heavily', 185), ('phantom', 185), ('pathetic', 184), ('seek', 184), ('freak', 184), ('magnificent', 184), ('recently', 184), ('therefore', 184), ('hill', 184), ('ron', 184), ('league', 184), ('al', 183), ('shocking', 183), ('norton', 183), ('las', 183), ('commentary', 183), ('sean', 183), ('lucas', 183), ('ultron', 183), ('industry', 182), ('camp', 182), ('richard', 182), ('teenager', 182), ('largely', 182), ('witness', 182), ('willing', 182), ('confused', 182), ('te', 182), ('sin', 182), ('inception', 182), ('turtle', 182), ('speech', 181), ('ant', 181), ('foot', 181), ('homage', 181), ('civil', 181), ('range', 180), ('han', 180), ('dynamic', 180), ('empty', 180), ('below', 180), ('achievement', 179), ('opposite', 179), ('sleep', 179), ('sacrifice', 179), ('page', 179), ('sexual', 179), ('antagonist', 178), ('wide', 178), ('gory', 178), ('perfection', 178), ('welcome', 178), ('bradley', 178), ('faithful', 178), ('wild', 178), ('demand', 178), ('badass', 178), ('wit', 178), ('ex', 178), ('utter', 178), ('debut', 177), ('nostalgia', 177), ('earn', 177), ('introduction', 177), ('device', 177), ('satire', 177), ('central', 177), ('blunt', 177), ('bible', 177), ('exact', 177), ('invest', 177), ('propaganda', 177), ('sing', 176), ('whatsoever', 176), ('hang', 176), ('distract', 176), ('grant', 176), ('knock', 175), ('encounter', 175), ('former', 175), ('improvement', 175), ('search', 175), ('slave', 175), ('split', 175), ('cash', 175), ('cameron', 175), ('grade', 174), ('information', 174), ('corny', 174), ('ahead', 174), ('aware', 174), ('occur', 174), ('fassbender', 174), ('wise', 173), ('bottom', 173), ('influence', 173), ('subplot', 173), ('instance', 173), ('prepare', 173), ('government', 173), ('teach', 172), ('tedious', 172), ('uma', 172), ('capable', 172), ('quiet', 172), ('driver', 172), ('slight', 172), ('nation', 172), ('refreshing', 172), ('andrew', 172), ('band', 172), ('russell', 172), ('wright', 172), ('dory', 172), ('qui', 171), ('ms', 171), ('commercial', 171), ('arrive', 171), ('assassin', 171), ('ultimate', 170), ('loki', 170), ('disturbing', 170), ('favor', 170), ('combat', 170), ('creator', 170), ('bizarre', 169), ('round', 169), ('player', 169), ('cringe', 169), ('loss', 169), ('stretch', 169), ('nightmare', 168), ('gain', 168), ('interaction', 168), ('stellar', 168), ('clint', 168), ('existence', 167), ('reflect', 167), ('remarkable', 167), ('blame', 167), ('tend', 167), ('reaction', 167), ('physical', 167), ('racist', 167), ('friendship', 166), ('surround', 166), ('worry', 166), ('suspect', 166), ('police', 166), ('load', 166), ('alright', 166), ('mel', 166), ('goal', 166), ('natalie', 166), ('proof', 166), ('breathtake', 166), ('emma', 166), ('allen', 165), ('traditional', 165), ('andy', 165), ('insane', 165), ('non', 165), ('documentary', 165), ('joe', 165), ('progress', 165), ('numerous', 164), ('pan', 164), ('nomination', 164), ('cloverfield', 164), ('regardless', 163), ('overrate', 163), ('silent', 163), ('board', 163), ('cage', 163), ('maintain', 163), ('code', 163), ('ian', 163), ('whedon', 163), ('v', 163), ('news', 163), ('elton', 163), ('grace', 162), ('masterful', 162), ('test', 162), ('pleasure', 162), ('watchable', 162), ('prince', 162), ('dare', 162), ('heck', 162), ('shake', 162), ('intensity', 162), ('creation', 162), ('gag', 162), ('fool', 162), ('hardy', 162), ('chappie', 162), ('amusing', 161), ('est', 161), ('clue', 161), ('ambitious', 161), ('account', 161), ('system', 161), ('vol', 161), ('appropriate', 160), ('politic', 160), ('layer', 160), ('combination', 160), ('center', 160), ('similarity', 160), ('context', 160), ('pretend', 160), ('grey', 160), ('suicide', 160), ('shaun', 160), ('scarlett', 160), ('stereotype', 159), ('exceptional', 159), ('quote', 159), ('esta', 159), ('plane', 159), ('quentin', 159), ('harsh', 159), ('leonardo', 159), ('dreamwork', 159), ('angle', 158), ('fascinating', 158), ('remove', 158), ('ugly', 158), ('depressing', 158), ('artist', 158), ('silver', 158), ('mar', 158), ('terrify', 158), ('fairy', 158), ('stewart', 158), ('lane', 158), ('cross', 157), ('belief', 157), ('weight', 157), ('singing', 157), ('moviegoer', 157), ('jane', 157), ('religious', 157), ('define', 156), ('separate', 156), ('flesh', 156), ('pour', 156), ('pas', 156), ('proper', 156), ('lay', 156), ('makeup', 156), ('intrigue', 156), ('accomplish', 156), ('student', 156), ('hundred', 156), ('wood', 156), ('winter', 156), ('dazzle', 155), ('faith', 155), ('storm', 155), ('husband', 155), ('snow', 155), ('trope', 155), ('nominate', 155), ('manner', 155), ('vampire', 155), ('wake', 155), ('private', 155), ('imaginative', 154), ('legend', 154), ('attitude', 154), ('district', 154), ('merely', 154), ('warn', 154), ('engaging', 154), ('contrast', 154), ('eddie', 154), ('johansson', 154), ('tragic', 153), ('terribly', 153), ('folk', 153), ('disgusting', 153), ('uninteresting', 153), ('steel', 153), ('relatively', 153), ('muy', 153), ('hair', 153), ('absurd', 153), ('maguire', 153), ('clone', 153), ('bird', 153), ('hunger', 153), ('fury', 153), ('season', 152), ('gold', 152), ('necessarily', 152), ('limit', 152), ('argue', 152), ('elysium', 152), ('unknown', 151), ('ed', 151), ('task', 151), ('kyle', 151), ('setup', 151), ('menace', 151), ('wick', 151), ('surpass', 150), ('leader', 150), ('tragedy', 150), ('imagery', 150), ('sandler', 150), ('abram', 150), ('aronofsky', 150), ('freeman', 150), ('likeable', 149), ('flashback', 149), ('criminal', 149), ('exceed', 149), ('roger', 149), ('expand', 149), ('meaningful', 149), ('casino', 149), ('philosophical', 149), ('mi', 149), ('survival', 149), ('joel', 148), ('du', 148), ('destruction', 148), ('unfold', 148), ('golden', 148), ('topic', 148), ('delight', 148), ('expression', 148), ('crowe', 148), ('emily', 148), ('dry', 147), ('pero', 147), ('twilight', 147), ('haunt', 147), ('furious', 147), ('innocent', 146), ('amazingly', 146), ('si', 146), ('mental', 146), ('nod', 146), ('ethan', 146), ('mainstream', 146), ('jean', 146), ('greatly', 146), ('brilliance', 145), ('colorful', 145), ('bien', 145), ('shell', 145), ('sharp', 145), ('jordan', 145), ('realise', 144), ('acclaim', 144), ('rubbish', 144), ('site', 144), ('fate', 144), ('repetitive', 144), ('essence', 144), ('halfway', 144), ('protect', 144), ('evan', 144), ('hugo', 144), ('jk', 144), ('sixth', 143), ('magneto', 143), ('paint', 143), ('whom', 143), ('cliched', 143), ('extraordinary', 143), ('skull', 143), ('pile', 142), ('childish', 142), ('spacey', 142), ('crude', 142), ('path', 142), ('phone', 142)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA5LQ6aXxuhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize(some_string, vocab):\n",
        "  ### Answer starts here ###\n",
        "  review = some_string.split()\n",
        "  review_vector = np.zeros(len(vocab))\n",
        "  for word in review:\n",
        "    for index in range(len(vocab)):\n",
        "      if word == vocab[index][0]:\n",
        "        review_vector[index] = 1\n",
        "  return review_vector\n",
        "  ### Answer ends here ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKzkN5wI7m7W",
        "colab_type": "text"
      },
      "source": [
        "Test your function with the following input.\n",
        "The vector should have four \"1\"s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH-ElttQyUEl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "793bcffc-9242-4c88-d3e7-c9eab7a6ac72"
      },
      "source": [
        "vector = vectorize(\"the and a of zyxw\", vocabulary)\n",
        "print(vector)\n",
        "print(sum(vector))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 0. 0. ... 0. 0. 0.]\n",
            "4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSanDv078IPM",
        "colab_type": "text"
      },
      "source": [
        "Now, vectorize the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQvKksH9v9-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Answer starts here ###\n",
        "vectorized_data = vectorize_all([[review, score] for review, score in clean_data], vocabulary)\n",
        "### Answer ends here ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNeZJijsw8gh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "eed986d0-0a3d-4744-da9a-2bdd12b2b59d"
      },
      "source": [
        "for vector, score in random.sample(vectorized_data, 5):\n",
        "  print_review(vector, score)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------- Review with score of 9 ---------------\n",
            "[1. 1. 1. ... 0. 0. 0.]\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 2 ---------------\n",
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 10 ---------------\n",
            "[1. 1. 1. ... 0. 0. 0.]\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 9 ---------------\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 6 ---------------\n",
            "[1. 1. 1. ... 0. 0. 0.]\n",
            "------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzNo-5oNRXz6",
        "colab_type": "text"
      },
      "source": [
        "For convenience, we will write a function called `preprocess_sample_point` which takes as input a single raw review and ouputs its binary bag-of-words representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT5GwjOBRv3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sample_point(review, vocab):\n",
        "  return vectorize(clean(review), vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX84q4B5SBfy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b42570e5-631e-45bb-cdb2-1f09051f52eb"
      },
      "source": [
        "vectorized_review = preprocess_sample_point(\n",
        "    'The movie was not bad, it was really good!', vocabulary)\n",
        "print(sum(vectorized_review))\n",
        "print(vectorized_review)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.0\n",
            "[1. 1. 1. ... 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri1gwmUjvJdT",
        "colab_type": "text"
      },
      "source": [
        "### Question 2.4 Cleaning, Again\n",
        "\n",
        "You may have noticed that some of the reviews are not in English. If we keep them in the dataset, this may cause accuracy to decrease. This is due to the fact that many of them will contain no words from our vocabulary, and will all have a zero-vector bag-of-words representation. However, there may be non-English language reviews that contain at least one word which is spelled the same as an English word. So, a better heuristic to determine if a review is in English or not is to check if it has at least 5 words from our vocabulary.\n",
        "\n",
        "To that end, create a variable called `english_data`, which contains only the bag-of-words representations of english reviews by keeping only those which have at least 5 non-zero entries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1wTcINxyiQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Answer starts here ### \n",
        "english_data = [[review, score] for review, score in vectorized_data if sum(review) > 4]\n",
        "### Answer ends here ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUrFxn1VysEy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4abca4e5-68c3-4566-fd7f-d211496b458d"
      },
      "source": [
        "print(len(vectorized_data), len(english_data))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39758 38955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu6X3BTYaGL3",
        "colab_type": "text"
      },
      "source": [
        "## 3. Train-Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs_t1uVzh01s",
        "colab_type": "text"
      },
      "source": [
        "### Question 3.1 Shuffling\n",
        "\n",
        "The dataset is not ordered randomly. In fact, reviews from the same movie are put one after another as well as movies from the same year. To avoid introducing bias when splitting the dataset, we should shuffle it.\n",
        "\n",
        "To that end, first create a copy of the variable `english_data`, which will be called `shuffled_data`, and then shuffle it so that it consists of a random reordering of the rows in `english_data`. \n",
        "\n",
        "**Note**: In practice, making a copy is not necessary. This is done to make testing easier since Jupyter alters global state whenever a cell is executed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6veN47b2kv8N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "9b5c38eb-c254-4f59-aceb-4380aa6446b9"
      },
      "source": [
        "# We create a copy of the clean_data\n",
        "### Answer starts here ###\n",
        "shuffled_data = eng_data.copy()\n",
        "### Answer ends here ###\n",
        "\n",
        "# We will check the first 5 reviews, non-randomized\n",
        "for review, score in english_data[0:5]:\n",
        "  print_review(review, sum(review))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------- Review with score of 21.0 ---------------\n",
            "[1. 1. 1. ... 0. 0. 0.]\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 17.0 ---------------\n",
            "[1. 1. 1. ... 0. 0. 0.]\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 27.0 ---------------\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 20.0 ---------------\n",
            "[1. 1. 1. ... 0. 0. 0.]\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 60.0 ---------------\n",
            "[1. 1. 1. ... 0. 0. 0.]\n",
            "------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppMCjpKgjWNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Randomly reorder `shuffled_data`, making sure that `clean_data` is not altered\n",
        "### Answer starts here ###\n",
        "random.shuffle(shuffled_data)\n",
        "\n",
        "### Answer ends here ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3ohA8IZmah8",
        "colab_type": "code",
        "outputId": "ae6d2afa-b856-410d-ba42-b62675ff7a88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Let's check the first 5 movies in the randomized list\n",
        "for review, score in shuffled_data[0:5]:\n",
        "  print_review(review, score)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------- Review with score of 9 ---------------\n",
            "[1. 1. 1. ... 0. 0. 0.]\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 10 ---------------\n",
            "[1. 1. 1. ... 0. 0. 0.]\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 10 ---------------\n",
            "[1. 1. 1. ... 0. 0. 0.]\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 7 ---------------\n",
            "[1. 1. 1. ... 0. 0. 0.]\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 2 ---------------\n",
            "[1. 1. 1. ... 0. 0. 0.]\n",
            "------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl9Z0VVXoRsY",
        "colab_type": "text"
      },
      "source": [
        "### Question 3.2 Creating Feature and Class variables\n",
        "\n",
        "Now that the data is shuffled, we will create a variable that will contain all the features, called `X` and another which will contain all the classes, called `y`.\n",
        "\n",
        "Further, to simplify the task, we will use 2 classes intead of 11. That is, review scores 0, 1, 2, 3 will be class 0, review scores 9, 10 will be given class 1, and everything else will be ignored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFI694mJVpcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Answer start here ###\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "#only retain the reviews with scores 0,1,2,3,9,10. Ignore the rest.\n",
        "for review, score in shuffled_data:\n",
        "  if (score == 1 or score == 2 or score == 3 or score == 0):\n",
        "    X.append(review)\n",
        "    y.append(0)\n",
        "  if (score == 9 or score == 10):\n",
        "    X.append(review)\n",
        "    y.append(1)\n",
        "### Answer ends here ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZgW68as2ut1",
        "colab_type": "text"
      },
      "source": [
        "Let's check the class balance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BRGec9y2ytw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "67df95ca-ccb1-49bb-9944-964fccd9a39e"
      },
      "source": [
        "for i in range(2):\n",
        "  print('class {}:'.format(i), \n",
        "        100 * len([0 for value in y if value == i])/len(y))\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class 0: 32.43806720627913\n",
            "class 1: 67.56193279372087\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NktoXfyT21k0",
        "colab_type": "text"
      },
      "source": [
        "We have to keep in mind that most of the dataset is made out of labels which are 1. For instance, if a classifier always outputs 1, they would have an accuracy of 67%. Thus, 67% should be our baseline performance, not 50%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Perywb8YapEU",
        "colab_type": "text"
      },
      "source": [
        "### Question 3.3 Spliting the Dataset\n",
        "\n",
        "Partition the dataset into a test set and a training dataset. Select 80% of the data to be part of the training set and the rest to be part of the test set.\n",
        "\n",
        "To that end, write a function called `train_test_split` which\n",
        "takes as input `X`, `y` and `test_size`, a floating point number between 0 and 1indicating how much of the dataset should be part of the test set. The function should return a tuple in the order:\n",
        "\n",
        "`(X_train, X_test, y_train, y_test)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHET6MzjhW-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_split(X, y, train_size):\n",
        "  ### Answer starts here ###\n",
        "  index = int(train_size * len(X))\n",
        "  return X[:index], X[index:], y[:index], y[index:]\n",
        "  ### Answer ends here ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH3mpjTHp3a-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, 0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-azU55RKk-s",
        "colab_type": "text"
      },
      "source": [
        "## 4. Support Vector Machines\n",
        "\n",
        "A support vector classifier tries to find the best separating hyperplane through the data. If the data is linearly separable, it finds the a hyperplane which maximized the margin. If it isn't it tries to minimize cost associated with misclassifying points.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKLBuWjmAKoJ",
        "colab_type": "text"
      },
      "source": [
        "### Question 4.1 Creating a Support Vector Classifier\n",
        "Using `scikit-learn`, create a support vector classifier for our review data.\n",
        "\n",
        "1. Use `scikit-learn` to create a linear support vector classifer\n",
        "2. Fit the model to our training set\n",
        "3. Print training accuracy\n",
        "4. Print test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11h5qZgArCLb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6925501a-a79a-4204-9995-791c27660def"
      },
      "source": [
        "### Answer starts here ###\n",
        "svm_clf = SVC(kernel='linear')\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training Accuracy: {}\".format(accuracy_score(y_train, svm_clf.predict(X_train))))\n",
        "print(\"Test Accuracy: {}\".format(accuracy_score(y_test, svm_clf.predict(X_test))))\n",
        "### Answer ends here ###"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.9552864224027799\n",
            "Test Accuracy: 0.9021050480277948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRk2DxHptzmE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6b2d5b66-7eaf-4809-8b17-b99ec0b34e99"
      },
      "source": [
        "print(svm_clf.predict([preprocess_sample_point(\n",
        "    'Boring. Such a bad movie. It was terrible and predictable', vocabulary)]))\n",
        "\n",
        "print(svm_clf.predict([preprocess_sample_point(\n",
        "    'I really liked this movie, it\\'s great!', vocabulary)]))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\n",
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_9sFSUNKscM",
        "colab_type": "text"
      },
      "source": [
        "## 5. Random Forests\n",
        "\n",
        "Random forests are a kind of ensemble classifier, i.e. they are made up of a number of 'weak' learners where the final classification is a combination of the classifications of each learner. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myn-42J9ACsH",
        "colab_type": "text"
      },
      "source": [
        "### Question 5.1 Creating a Random Forest Classifier\n",
        "Using `scikit-learn`, create a radom forest classifier for our review data.\n",
        "\n",
        "1. Use `scikit-learn` to create a random forest classifier\n",
        "2. Fit the model to our training set\n",
        "3. Print training accuracy\n",
        "4. Print test accuracy\n",
        "\n",
        "\n",
        "\n",
        "Be sure to check the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Try to play around with the hyperparameters to see if you can get higher accuracy. Specifically, try finding good values for `n_estimators`, `min_samples_split`, `max_depth` and `max_features`. Try to beat the linear SVM's accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddm3xZiNZkXu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b2b6de85-047e-4088-9632-f6d0d0408d26"
      },
      "source": [
        "### Answer starts here ###\n",
        "rfc = RandomForestClassifier(n_estimators=10, min_samples_split=2, max_depth=None, max_features='auto')\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training Accuracy: {}\".format(accuracy_score(y_train, rfc.predict(X_train))))\n",
        "print(\"Test Accuracy: {}\".format(accuracy_score(y_test, rfc.predict(X_test))))\n",
        "\n",
        "### Answer ends here ###"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.9944299657621749\n",
            "Test Accuracy: 0.8506029021050481\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPcnt6mI2g5e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "356d3306-2453-458a-8bcc-9125a11c2d23"
      },
      "source": [
        "print(rfc.predict([preprocess_sample_point(\n",
        "    'Boring. This movie is terrible', vocabulary)]))\n",
        "\n",
        "print(rfc.predict([preprocess_sample_point(\n",
        "    'This movie was pretty good', vocabulary)]))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\n",
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T__chgNKukq",
        "colab_type": "text"
      },
      "source": [
        "##6. Naive Bayes\n",
        "Until now, we've used pre-existing implementations of two types of classifiers: random forests and support vector machines. Now, we will cover a third type of classifier: naive Bayes classifers. However, this time we will be writing it from scratch.\n",
        "\n",
        "Naive Bayes classifiers are part of a larger family of classifiers which are so called 'probabilistic classifiers'. This is because they do not only try to predict classes given features, but they estimate probabilities distributions over a set of classes.\n",
        "\n",
        "First, we will go over some definitions.\n",
        "\n",
        "**Definition.** A *prior probability* is the likelihood of an event given no further assumptions. For instance, the probability that it's raining is relatively low.\n",
        "\n",
        "**Definiton.** A *posterior probability* or *conditional probability* is the likelihood of an event given that some other event has occurred. For instance, the probability that it's raining given that there are clouds is higher than if we don't make that assumption.\n",
        "\n",
        "Now we will go over some motivation.\n",
        "\n",
        "For the purpose of argument, imagine we had access to the probability distribution $\\Pr$. That is, we know how likely features and classes are. For example, $\\Pr(x_1 = 1)$ is the probability that the most common word, i.e. \"the\", is in a random movie review. Presumably, this probability is relatively high. As a second example, $\\Pr(y = 1)$ is the probability that a random movie review is 'good'. In our case, this would be somewhere close to 0.67.\n",
        "\n",
        "Since we hypothetically have access to the whole probability distribution, we also know conditional probabilities. For instance, we would know $\\Pr(y = 0 \\; \\vert \\; x_1 = 0)$, which is the probability that a random review is 'bad', given that it does not contain the word \"the\".\n",
        "\n",
        "Given a probability distribution, we can find an optimal classifier which simply picks the class which maximizes the probability that we will see that class given the observed features, in other words our classifier $f: \\B^n \\to \\B$ is given by:\n",
        "\n",
        "$$ f(x_1, \\ldots, x_n) = \\argmax_{c \\in \\B} \\Pr(y = c \\given x_1, \\ldots, x_n ).$$\n",
        "\n",
        "Where $\\argmax$ returns the element in $\\B$ which maximizes the expression to its right, and $\\B$ is the set with two elements, $\\{0, 1\\}$. For example, we have\n",
        "$$ \\argmax_{x \\in \\R} (x - x^2) = \\frac 1 2, $$\n",
        "since $\\frac 1 2$ maximizes the expression $x - x^2$.\n",
        "\n",
        "It would be great if we had access to the probability distribution $\\Pr$, but unfortunately we don't in almost every case. This means we wish to try to estimate it given some samples, i.e. the training data.\n",
        "\n",
        "However, we run into another issue: estimating the probability distribution is computionally expensive. Therefore, we assume that the different features are independent from one another. This is called the *naive conditional independence assumption*. In other words, we assume that\n",
        "\n",
        "$$ \\forall i \\in \\{1, \\ldots, n\\} : \\Pr (x_i \\given y, x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n) = \\Pr(x_i \\given y).$$\n",
        "\n",
        "Using Bayes' Theorem, we can simplify the conditional independence assumption to:\n",
        "\n",
        "$$\\Pr(y \\given x_1, \\ldots, x_n) = \\frac{\\Pr(y) \\prod_{i=1}^n \\Pr(x_i \\given y)}{\\Pr(x_1, \\ldots, x_n)}.$$\n",
        "\n",
        "However, we can observe that the denominator is constant for a given input, so it's not actually necesarry to estimate it if all we want is to find the class with the maximum posterior probability. In other words,\n",
        "\n",
        "$$ \\Pr(y \\given x_1, \\ldots, x_n) \\propto \\Pr(y) \\prod_{i=1}^n \\Pr(x_i \\given y), $$\n",
        "\n",
        "so, our classification rule becomes\n",
        "\n",
        "$$ f(x_1, \\ldots, x_n) = \\argmax_{y \\in \\B} \\Pr(y) \\prod _{i=1}^n \\Pr(x_i \\given y).$$\n",
        "\n",
        "Where $\\propto$ means \"proportional to\" and  $\\prod_{i = 1}^n g(i)$ is like summation $\\left(\\sum_{i=1}^n g(i)\\right)$, except that addition is replaced with multiplication. For example,\n",
        "\n",
        "$$\\prod_{i = 1}^5 i^2 = 1^2 \\cdot 2^2 \\cdot 3^2 \\cdot 4^2 \\cdot 5^2.$$\n",
        "\n",
        "**Note**: To estimate prior and conditional probabilities, we use the ratios of occurence counts found in the dataset. For example, to estimate $\\Pr(x_1 = 0 \\; \\vert \\; y = 0)$, we have to calculate the number of instances of class zero for which $x_1 = 0$ and divide them by the number of instances of class 0.\n",
        "\n",
        "**Note**: The naive independence assumption is usually false in practice for most features. Therefore, the resulting estimated probability distribution is usually a bad approximation of the true distribution. However, the resulting classifier often has a good performance, depending on the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN-dse1NBQnk",
        "colab_type": "text"
      },
      "source": [
        "### Quesiton 6.1 Estimating the Probability Distribution\n",
        "\n",
        "It would be expensive to re-estimate prior and posterior probabilities every time, so we should save probabilities in memory.\n",
        "\n",
        "Thus, you will need to save\n",
        "1. $\\Pr(y)$ for each $y \\in \\B$, and\n",
        "2. $\\Pr(x_i = u \\; \\vert \\; y)$ for each $ i \\in \\{1, \\ldots, n\\}$, $u \\in \\mathbb{B}$ and $y \\in \\mathbb{B}$.\n",
        "\n",
        "Remember that you are *estimating* the probabilities using the training set only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-KrWs3DER_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Answer starts here ###\n",
        "#Probability of y for each y in {0, 1\n",
        "prob_y = []\n",
        "prob_y.append(y_train.count(0)/len(y_train))\n",
        "prob_y.append(y_train.count(1)/len(y_train))\n",
        "\n",
        "#Probability of each x given y\n",
        "prob_x_given_y = []\n",
        "\n",
        "# for each word\n",
        "for i in range(num_features):\n",
        "  count_positive = 0\n",
        "  count_negative = 0\n",
        "  prob_for_word = []\n",
        "  for j, X in enumerate(X_train):\n",
        "    # if a specific word X[i] is present in the review and the review is positive P(X⋂y1), add one to the word's positive count\n",
        "    if X[i] == 1 and y_train[j] == 1:\n",
        "      count_positive += 1\n",
        "    # if a specific word X[i] is present in the review and the review is negative P(X⋂y0), add one to the word's negative count\n",
        "    if X[i] == 1 and y_train[j] == 0:\n",
        "      count_negative += 1\n",
        "  \n",
        "  #Append P(X|y) = P(X⋂y)/P(y) for y in {0,1\n",
        "  prob_for_word.append(count_negative/y_train.count(0))\n",
        "  prob_for_word.append(count_positive/y_train.count(1))\n",
        "  prob_x_given_y.append(prob_for_word)\n",
        "### Answer ends here ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfCjr-JrEJya",
        "colab_type": "text"
      },
      "source": [
        "### Question 6.2 Creating the Naive Bayes Classifier\n",
        "\n",
        "Create a function called `naive_bayes` which will take as input a list of features $x_1, \\ldots, x_n$ and outputs the class with the largest posterior probability given the input features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfKW9cZs4x-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def naive_bayes(vec):\n",
        "  ### Answer starts here ###\n",
        "  # set variables from above as global (to access them in a function)\n",
        "  global prob_y\n",
        "  global prob_x_given_y\n",
        "  \n",
        "  list_of_probs = []\n",
        "  \n",
        "  #for 0 (negative) and 1 (positive)\n",
        "  for i in range(len(prob_y)):\n",
        "    x_given_y = 1\n",
        "    for j in range(len(vec)):\n",
        "      # for all the words present in the input list\n",
        "      if vec[j] == 1:\n",
        "        # get the corresponding probability of x given y from above variables\n",
        "        # and add it to the multiplication terms ∏\n",
        "        x_given_y *= prob_x_given_y[j][i]\n",
        "        \n",
        "    list_of_probs.append(x_given_y * prob_y[i])\n",
        "  \n",
        "  #find the highest probability between positive and negative \n",
        "  if list_of_probs[0] > list_of_probs[1]:\n",
        "    return 0\n",
        "  else: \n",
        "    return 1\n",
        "  ### Answer ends here ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1iGVZtkE5fF",
        "colab_type": "text"
      },
      "source": [
        "### Question 6.3 Measuring Performance\n",
        "\n",
        "Using the naive Bayes classifier, predict the classes for each sample point in the training set as well as the test set and print accuracies.\n",
        "\n",
        "**Note.** You should get train and test accuracies of about 84-85%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SjmIjYPSW10",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c6d49b18-6d4a-47a8-db90-82a7adc15a11"
      },
      "source": [
        "### Answer starts here ###\n",
        "train_preds = []\n",
        "for X in X_train:\n",
        "  train_preds.append(naive_bayes(X))\n",
        "\n",
        "test_preds = []\n",
        "for X in X_test:\n",
        "  test_preds.append(naive_bayes(X))\n",
        "\n",
        "# accuracy function\n",
        "def accuracy(predictions, labels):\n",
        "  correct = 0\n",
        "  for index in range(len(predictions)):\n",
        "    if (predictions[index] == labels[index]):\n",
        "      correct += 1\n",
        "  return correct/len(predictions)\n",
        "\n",
        "print(accuracy(train_preds, y_train))\n",
        "print(accuracy(test_preds, y_test))\n",
        "### Answer ends here ###"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7497572691501865\n",
            "0.7557735540568159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2ZbGmYj2WnZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6f6cd41d-b6fa-4401-ba18-54c537f658d6"
      },
      "source": [
        "print(rfc.predict([preprocess_sample_point(\n",
        "    'Boring. This movie is terrible', vocabulary)]))\n",
        "\n",
        "print(rfc.predict([preprocess_sample_point(\n",
        "    'This movie was pretty good', vocabulary)]))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\n",
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7okgCQqFnxm",
        "colab_type": "text"
      },
      "source": [
        "## 7. Conclusion\n",
        "\n",
        "You have successfully completed MAIS 202 assignment 2! Hope you enjoyed! To submit, download as .py and upload it to its respective okpy assignment."
      ]
    }
  ]
}